{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- University: University of SÃ£o Paulo (USP) \n",
    "\n",
    "- Class: PMR3508 (2021) - Fundamentals of Machine Learning\n",
    "\n",
    "- Kaggle Competition: Adult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "1. [Setup and imports](#Setup-and-imports)\n",
    "    1. [Libraries](##Libraries)\n",
    "    2. [Setup](##Setup)\n",
    "2. [EDA (Exploratory Data Analysis)](#EDA(Exploratory-Data-Analysis))\n",
    "    1. [Glance at data](##Glance-at-data)\n",
    "        1. [Train dataset](###Train-dataset)\n",
    "        2. [Test dataset](###Test-dataset)\n",
    "    2. [Summary statistics](##Summary-statistics)\n",
    "    3. [Target histogram](##Target-histogram)\n",
    "    4. [Non zero counts](##Non-zero-counts)\n",
    "    5. [Empirical distribution of features](##Empirical-distribution-of-features)\n",
    "        1. [Train dataset](###Train-dataset)\n",
    "            1. [Histograms of numerical features](####Histograms-of-numerical-features)\n",
    "            2. [Bar plots for categorical features](####Barplots-for-categorical-features)\n",
    "        2. [Test dataset](###Test-dataset)\n",
    "            1. [Histograms of numerical features](####Histograms-of-numerical-features)\n",
    "            2. [Bar plots for categorical features](####Barplots-for-categorical-features)\n",
    "        3. [Plots of target vs features](###Plots-of-target-vs-features)\n",
    "            1. [Numerical features](####Numerical-features)\n",
    "            2. [Categorical features](####Categorical-features)\n",
    "        4. [Pairwise plots](###Pairwise-plots)\n",
    "            1. [Scatter plot](####Numerical-vs-numerical)\n",
    "            2. [Correlation heatmap](####Correlation-heatmap)\n",
    "            3. [Categorical heatmap](####Categorical-heatmap)\n",
    "3. [Data engineering](#Data-engineering)\n",
    "    1. [Divide dataset into numerical and categorical subdatasets](##Divide-dataset-into-numerical-and-categorical-subdatasets)\n",
    "    1. [Normalize features](##Normalize-features)\n",
    "    2. [Treat categorical features](##Treat-categorical-features)\n",
    "    3. [Joining numerical and categorical dfs back](##Joining-numerical-and-categorical-dfs-back)\n",
    "    4. [Treat missing values](##Treat-missing-values)\n",
    "    5. [Treat outliers](##Treat-outliers)\n",
    "    6. [Feature tranformations](##Feature-tranformations)\n",
    "    7. [Mirror on test dataset](##Mirror-on-testdataset)\n",
    "4. [Feature Engineering](#Featur-engineering)\n",
    "    1. [Importance sampling](##Importance-sampling)\n",
    "    2. [Select features](##Select-features)\n",
    "    3. [Create new features](##Create-new-features)\n",
    "5. [Experiments](#Experiments)\n",
    "    1. [Base dataset](##Base-dataset)\n",
    "    2. [Baseline (KNN)](##Baseline-(KNN))\n",
    "    3. [4 Classifiers](##4-Classifiers)\n",
    "        1. [RF](###RF)\n",
    "        2. [XGBoost](###XGBoost)\n",
    "        3. [SVM](###SVM)\n",
    "        4. [NN](###NN)\n",
    "    4. [Engineered datasets](##Engineered-datasets)\n",
    "6. [Final model](#Final-model)\n",
    "7. [Submission](#Submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and imports\n",
    "### Setup environment and import libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "py36 not influencing\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import copy\n",
    "\n",
    "# Just because I have a conflicting Python 3.6 installation at root\n",
    "# Comment this when uploading to kaggle\n",
    "try:\n",
    "    sys.path.remove('C:/Python36/Lib/site-packages')\n",
    "    sys.path.remove('C:/Python36/Lib')\n",
    "except:\n",
    "    print(\"py36 not influencing\")\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "sns.set()\n",
    "\n",
    "# Loading data\n",
    "adultTrain = pd.read_csv(\n",
    "    \"C:/Users/bruno/Desktop/kaggle-adult-comp-knn/data/train_data.csv\",\n",
    "    sep=r'\\s*,\\s*',\n",
    "    engine='python',\n",
    "    na_values=\"?\",\n",
    ")\n",
    "\n",
    "# # For uploading to kaggle\n",
    "# adultTrain = pd.read_csv(\n",
    "#     \"/kaggle/input/adult-pmr3508/train_data.csv\",\n",
    "#     sep=r'\\s*,\\s*',\n",
    "#     engine='python',\n",
    "#     na_values=\"?\",\n",
    "# )\n",
    "\n",
    "adultTest = pd.read_csv(\n",
    "    \"C:/Users/bruno/Desktop/kaggle-adult-comp-knn/data/test_data.csv\",\n",
    "    sep=r'\\s*,\\s*',\n",
    "    engine='python',\n",
    "    na_values=\"?\",\n",
    ")\n",
    "\n",
    "# # For uploading to kaggle\n",
    "# adultTest = pd.read_csv(\n",
    "#     \"/kaggle/input/adult-pmr3508/test_data.csv\",\n",
    "#     sep=r'\\s*,\\s*',\n",
    "#     engine='python',\n",
    "#     na_values=\"?\",\n",
    "# )\n",
    "\n",
    "modifyNames = {\n",
    "    \"fnlwgt\": \"weight\",\n",
    "    \"education.num\": \"educationNum\", \n",
    "    \"marital.status\": \"maritalStatus\",\n",
    "    \"capital.gain\": \"capitalGain\", \n",
    "    \"capital.loss\": \"capitalLoss\",\n",
    "    \"hours.per.week\": \"hoursPerWeek\", \n",
    "    \"native.country\": \"country\",\n",
    "    \"income\": \"target\"\n",
    "}\n",
    "\n",
    "# Changing columns names\n",
    "adultTrain.rename(columns=modifyNames, inplace=True)\n",
    "adultTest.rename(columns=modifyNames, inplace=True)\n",
    "\n",
    "# Casting appropriate datatypes\n",
    "dtypes = {\n",
    "    \"age\": int,\n",
    "    \"workclass\": str,\n",
    "    \"weight\": int,             \n",
    "    \"education\": str,\n",
    "    \"educationNum\": int,\n",
    "    \"maritalStatus\": str,\n",
    "    \"occupation\": str,\n",
    "    \"relationship\": str,\n",
    "    \"race\": str,\n",
    "    \"sex\": str,\n",
    "    \"capitalGain\": int,\n",
    "    \"capitalLoss\": int,\n",
    "    \"hoursPerWeek\": int,\n",
    "    \"country\": str,\n",
    "    \"target\": str\n",
    "}\n",
    "\n",
    "adultTrain.astype(dtypes, copy=False)\n",
    "adultTest.astype(dtypes.pop(\"target\"), copy=False)\n",
    "\n",
    "# Id is not relevant, so it is dropped\n",
    "adultTrain.pop(\"Id\")\n",
    "idTest = adultTest.pop(\"Id\")\n",
    "\n",
    "# weight is not important for testing\n",
    "weightTrain = adultTrain[\"weight\"]\n",
    "adultTest.pop(\"weight\")\n",
    "\n",
    "print(\"\\n\\n#### TRAIN DATASET ####\")\n",
    "# (32560, 16)\n",
    "print('\\nshape: ', adultTrain.shape)\n",
    "# all as objects, need to change some datatypes\n",
    "print('\\ndata types:\\n', adultTrain.dtypes)\n",
    "# max of 4000 datapoints with some nan entry -> treat them\n",
    "print('\\nNumber of null entries:\\n', adultTrain.isnull().sum())\n",
    "# No duplicated data points\n",
    "print('\\nDuplicated data points:\\n', adultTrain.duplicated().sum()) \n",
    "\n",
    "print(\"\\n\\n#### TEST DATASET ####\")\n",
    "# (16280, 15)\n",
    "print('\\nshape: ', adultTest.shape)\n",
    "# all as objects, need to change some datatypes\n",
    "print('\\ndata types:\\n', adultTest.dtypes)\n",
    "# max of aprox 2000 datapoints with some nan entry -> treat them\n",
    "print('\\nNumber of null entries:\\n', adultTest.isnull().sum())\n",
    "# No duplicated data points\n",
    "print('\\nDuplicated data points:\\n', adultTest.duplicated().sum()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA (Exploratory Data Analysis)\n",
    "### Get to know data and draw insights on the problem of classifying income as > 50K."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glance at data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# education can be dropped, since educationNum is givving all the information we want\n",
    "# there is notinh specific about a certain degree that will affect the target\n",
    "adultTrain.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adultTest.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adultTrain.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Target histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aprox 25 000 datapoints <= 50K and 7 500 < 50K -> relatively imbalanced dataset\n",
    "# most simple baseline is prediciting always <= 50K -> gives 0.76% accuracy\n",
    "counts = adultTrain[\"target\"].value_counts().values\n",
    "imbalanceRatio = counts[0]/counts[1]\n",
    "print(imbalanceRatio)\n",
    "adultTrain[\"target\"].value_counts().plot(kind=\"bar\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non zero counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# capitalGain and capitalLoss have very few examples\n",
    "# ideas\n",
    "    # 1. exclude these festures\n",
    "    # 2. cluster them in two bins -> will become boolean variables\n",
    "print(adultTrain.astype(bool).sum(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot empirical distribution of each feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histograms of numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hoursPerWeek could be dividid in three bins:  <30, 30-50, >50\n",
    "# educationNUm could be dividid in four bins: <8, 8-10, 10-12, >13\n",
    "# capitalGains and capitalLoss needs to actuallt only form one feature \n",
    "# that is capitalLiquid = capitalGains - capitalLoss. \n",
    "# The effect of this feature will be almost as of a imbalanced binary variable since almost all values are zero\n",
    "# and the other are in a small range\n",
    "adultTrain.hist(bins=30, figsize=(15, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bar plots for categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Private is way bigger than the rest (therefore the rest of the classes have little data)\n",
    "# Without pay and never work have very few examples (14) but these examples guarantee we know the target\n",
    "# Ideas:\n",
    "    # 1. Cluster into 3 bins: private, {without pay + ever worked},  and rest -> \n",
    "    # but need to see if private and rest have distinct relatinships with target\n",
    "print('\"Without-pay\" or \"Never-worked\" datapoints: ', adultTrain[adultTrain[\"workclass\"] == (\"Without-pay\" or \"Never-worked\")].shape[0])\n",
    "adultTrain[\"workclass\"].value_counts().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This feature will be excluded, educationNum already gives us the info we need. There is nothing specific to a \n",
    "# certain category that would be relevant for predicting the target\n",
    "adultTrain[\"education\"].value_counts().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A priori a would think only having a present spouse or not is important\n",
    "# So this could be cluster into two groups: present spouse and not present spouse\n",
    "adultTrain[\"maritalStatus\"].value_counts().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each of the categories seem to be very important\n",
    "adultTrain[\"occupation\"].value_counts().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This feature seem a little weird, it doest provide mmuch new info, \n",
    "# and the categories dont seem to be mutually exclusive\n",
    "# Idea: exclude this feature\n",
    "adultTrain[\"relationship\"].value_counts().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this could be divided into two bins: white and black \n",
    "# because the rest doesnt have data and my guess they would be very similar to white\n",
    "adultTrain[\"race\"].value_counts().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a priori seems to be important\n",
    "adultTrain[\"sex\"].value_counts().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Surely maintaning all these low data categories will fit statistical noise and ruin the accuracy\n",
    "# Ideas: \n",
    "    # 1. divide in two bins: developed and not developed ccontries\n",
    "    # 2. divide in two bins: USA and rest\n",
    "adultTrain[\"country\"].value_counts().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histograms of numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no surprises here\n",
    "adultTest.hist(bins=30, figsize=(15, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bar plots for categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no surprises here\n",
    "adultTest[\"workclass\"].value_counts().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no surprises here\n",
    "adultTest[\"maritalStatus\"].value_counts().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no surprises here\n",
    "adultTest[\"occupation\"].value_counts().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no surprises here\n",
    "adultTest[\"race\"].value_counts().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no surprises here\n",
    "adultTest[\"country\"].value_counts().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots of target vs features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OBS: the plots below dont consider the dataset imbalaca, therefore, all ratios are essentially multiplied\n",
    "# by a factor of 2.3 in favour of <50K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for <30 it is almost certain that wage <50K; 30-40 roughly the same; 40-50 >50K has good advantage\n",
    "# <50K decays linearly with age, while 50K is like a normal function centered in 43\n",
    "sns.catplot(x=\"target\", y=\"age\", kind=\"violin\", inner=None, data=adultTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for <10 <50K has a good advantage; 10-12.5 same; >12.5 >50K has very good advantage\n",
    "sns.catplot(x=\"target\", y=\"educationNum\", kind=\"violin\", inner=None, data=adultTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for <40 <50K has a good advantage; for >40 >50K has a good advantage\n",
    "sns.catplot(x=\"target\", y=\"hoursPerWeek\", kind=\"violin\", inner=None, data=adultTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"target\", y=\"capitalGain\", kind=\"violin\", inner=None, data=adultTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"target\", y=\"capitalLoss\", kind=\"violin\", inner=None, data=adultTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OBS: I am multiplying the counts of >50K by the imbalaceRatio to decouple \n",
    "# the fact that the dataset is imbalaced from differences in distribution of the feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Private & Self-empinc differ a little, the rest is roughly the same, so can be grouoed \n",
    "# into a single category called other\n",
    "countsDf = adultTrain[[\"target\",\"workclass\"]].value_counts().unstack()\n",
    "countsDf.loc[\">50K\", :] = countsDf.loc[\">50K\", :]*imbalanceRatio\n",
    "countsDf.plot(kind=\"bar\", stacked=True,  figsize=(10, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all ctegories are different, thus maintaining all of them seems the way to go\n",
    "countsDf = adultTrain[[\"target\",\"maritalStatus\"]].value_counts().unstack()\n",
    "countsDf.loc[\">50K\", :] = countsDf.loc[\">50K\", :]*imbalanceRatio\n",
    "countsDf.plot(kind=\"bar\", stacked=True,  figsize=(10, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tranposrt moving, tech support, sales, creaf repair dont seem to help distringuish, so could be grouped\n",
    "# into a single category named rest\n",
    "countsDf = adultTrain[[\"target\",\"occupation\"]].value_counts().unstack()\n",
    "countsDf.loc[\">50K\", :] = countsDf.loc[\">50K\", :]*imbalanceRatio\n",
    "countsDf.plot(kind=\"bar\", stacked=True,  figsize=(10, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# black and other dimishes for over >50K but white dominates in both\n",
    "# I think grouping into white and non-white is a valid approach here\n",
    "countsDf = adultTrain[[\"target\",\"race\"]].value_counts().unstack()\n",
    "countsDf.loc[\">50K\", :] = countsDf.loc[\">50K\", :]*imbalanceRatio\n",
    "countsDf.plot(kind=\"bar\", stacked=True,  figsize=(10, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i think this can be mexico and non-mexico because the rest of the categories have so little data\n",
    "# that it is likely that we are fittng statistical noise\n",
    "countsDf = adultTrain[[\"target\",\"country\"]].value_counts().unstack()\n",
    "countsDf.loc[\">50K\", :] = countsDf.loc[\">50K\", :]*imbalanceRatio\n",
    "countsDf.plot(kind=\"bar\", stacked=True,  figsize=(15, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairwise plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical vs numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# age limmits >50K even with high education and hours per week\n",
    "# age < 35 seems to be good indicator -> could maybe be binary variable\n",
    "\n",
    "# capitalGain > aprox 5 000 seems to be a great separator \n",
    "# capitalGain > 50 000 guarantees >50K \n",
    "# could be categorical variable\n",
    "\n",
    "# educationNum > 10 seems to be good indicator also\n",
    " \n",
    "# 1 000 < capital loss < 3 000 can be good \n",
    "\n",
    "# hours per week < 50 good\n",
    "sns.pairplot(adultTrain, hue=\"target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all numerical features with very low correlation\n",
    "sns.heatmap(adultTrain.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adultTrainDummies = pd.get_dummies(adultTrain[[\"workclass\", \"maritalStatus\", \"occupation\", \"race\", \"country\"]])\n",
    "dummy_features = adultTrainDummies.columns.values\n",
    "pivots = []\n",
    "for feature in dummy_features:\n",
    "    rest_of_features = dummy_features[dummy_features != feature]\n",
    "    new_pivot = adultTrainDummies.groupby(feature)[rest_of_features].sum().fillna(0)\n",
    "    pivots.append(new_pivot)\n",
    "\n",
    "fullPivot = pd.concat(pivots)[dummy_features]\n",
    "fullPivotOnes = fullPivot.iloc[lambda x: x.index > 0]\n",
    "fullPivotOnes.set_index(adultTrainDummies.columns, inplace=True)\n",
    "\n",
    "def normalize_pivot_tables(fullPivot):\n",
    "    vec = np.array(fullPivot.sum(axis=1).values)\n",
    "    sizeDummies = vec.size\n",
    "    normMatrix = np.zeros((sizeDummies, sizeDummies))\n",
    "    for i, element in enumerate(vec):\n",
    "        for j, element2 in enumerate(vec):\n",
    "            normMatrix[i][j] = element + element2\n",
    "                        \n",
    "    normDf = pd.DataFrame(normMatrix, columns=fullPivot.columns)\n",
    "    normDf.set_index(fullPivot.columns, inplace=True)\n",
    "    fullPivotNorm = fullPivot.div(normDf)\n",
    "    return fullPivotNorm\n",
    "\n",
    "fullPivotNorm = normalize_pivot_tables(fullPivotOnes) # P(X1 = 1, X2 = 1)\n",
    "\n",
    "# dataset is too big, so will divide in two for plotting heatmaps\n",
    "#fullPivot2 = fullPivot.iloc[37:, :37] # down left -> not useful\n",
    "#fullPivot4 = fullPivot.iloc[37:, 37:] # down right -> country vs country -> not useful\n",
    "fullPivotNorm1 = fullPivotNorm.iloc[:37, :37] # top left\n",
    "fullPivotNorm3 = fullPivotNorm.iloc[:37:, 37:] # top right\n",
    "\n",
    "#OBS: dummy features with same prefix are mutually exclsusive, \n",
    "# therefore they will have joint prob equal to zero \n",
    "\n",
    "# max joint probability is aprox 0.1 in entire categorical combinations dataset, \n",
    "# therefore all categorical features are relatively independent from each other\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullPivotNorm.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(10,7))\n",
    "sns.heatmap(fullPivotNorm1,ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(10,7))\n",
    "sns.heatmap(fullPivotNorm3, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data engineering\n",
    "### Prepare data for algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide dataset into numerical and categorical subdatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numColumns = [\"age\", \"capitalGain\", \"capitalLoss\", \"educationNum\", \"hoursPerWeek\"] # obs: left weight out\n",
    "catColumns = [\"country\", \"education\", \"maritalStatus\", \"occupation\", \"race\", \"relationship\", \"sex\", \"workclass\"] # obs: left target out\n",
    "targetTrain = adultTrain[\"target\"]\n",
    "adultTrainNum = adultTrain[numColumns]\n",
    "adultTrainCat = adultTrain[catColumns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adultTrainNum.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adultTrainCat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adultTrainNum = (adultTrainNum-adultTrainNum.mean())/adultTrainNum.std()\n",
    "adultTrainNum.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treat categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target-encoding \n",
    "# encoder = TargetEncoder()\n",
    "# encoder.fit_transform(adultTrainCat, adultTrain[\"target\"])\n",
    "# Simple one-hot encoding (this will be chosen one for now)\n",
    "adultTrainCat = pd.get_dummies(adultTrainCat)\n",
    "adultTrainCat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining numerical and categorical dfs back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adultTrain = pd.concat([adultTrainNum, adultTrainCat], axis=1)\n",
    "adultTrain.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treat missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two main options\n",
    "# 1. Just thorw away rows with missing values\n",
    "# 2. Replace with mean of colummn (this will be chosen one for now)\n",
    "adultTrain.fillna(adultTrain.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treat outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature tranformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mirror on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adultTestNum = adultTest[numColumns]\n",
    "adultTestCat = adultTest[catColumns]\n",
    "\n",
    "adultTestNum = (adultTestNum-adultTestNum.mean())/adultTestNum.std() # broadcasts to columns by default\n",
    "\n",
    "adultTestCat = pd.get_dummies(adultTestCat)\n",
    "adultTestCat = adultTestCat.reindex(columns = adultTrainCat.columns, fill_value=0) # equivalent to fit transform\n",
    "\n",
    "adultTest = pd.concat([adultTestNum, adultTestCat], axis=1)\n",
    "\n",
    "adultTest.fillna(adultTest.mean(), inplace=True)\n",
    "\n",
    "adultTest.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "### Select and/or create new features. Non-linear transformations affect more KNN performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primising Engineered Datasets (v0)\n",
    "# Disct which will hold different feature engineered candidate datsets\n",
    "promisingDatasets = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importance sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### NOTEBOOK WAS TO SLOW WITH THIS, DECIDED TO COMMENT OUT ###################\n",
    "\n",
    "# # with the given weights, the rows can be resampled according to their weight\n",
    "# # number of rows = weight_factor*minMaxNormalized_weight where the weight factor is large enough so that the sampling\n",
    "# # can give different integer values for most of the rows, \n",
    "# # but if it is too large becomes computationally heavy\n",
    "# weightTrainNorm = ((weightTrain) - weightTrain.min())/(weightTrain.max() - weightTrain.min())\n",
    "# weightFactor = 50 # (can be altered later)\n",
    "# knnNeighboursFactor = weightTrainNorm.mean()*50 # expected value of number of columns added\n",
    "# print('knnNeighboursFactor:', knnNeighboursFactor)\n",
    "\n",
    "# # adultTrain70Importance will contain replicas only of row in it, will used to train KNN, \n",
    "# # that will be tested in adultTrain30ImportanceCV, which wasnt replicated\n",
    "# adultTrainShuffled = adultTrain.sample(frac=1)\n",
    "# adultTrain70Importance, adultTrain30Importance = \\\n",
    "#     np.split(adultTrainShuffled, [int(.7*len(adultTrain))])\n",
    "# adultTrain30ImportanceCopy = adultTrain30Importance.copy()\n",
    "# # putting back target I removed earlier\n",
    "# adultTrain70Importance[\"target\"] = targetTrain[:len(adultTrain70Importance)]\n",
    "\n",
    "# for idx, row in adultTrain70Importance.iterrows():\n",
    "#     numReplicatedRows = int(weightTrainNorm[idx]*weightFactor)\n",
    "#     df = row.to_frame().T\n",
    "#     adultTrain70Importance = adultTrain70Importance.append([df]*numReplicatedRows, ignore_index=True)\n",
    "    \n",
    "# for idx, row in adultTrain30ImportanceCopy.iterrows():\n",
    "#     numReplicatedRows = int(weightTrainNorm[idx]*weightFactor)\n",
    "#     df = row.to_frame().T\n",
    "#     adultTrain30ImportanceCopy = adultTrain30ImportanceCopy.append([df]*numReplicatedRows, ignore_index=True)\n",
    "\n",
    "# #### IMPORTANT: this is 100% of the actual training dataset -> only used if survived CV\n",
    "# adultTrainImportance = pd.concat([adultTrain70Importance, adultTrain30ImportanceCopy])\n",
    "# promisingDatasets[\"importanceSampling\"] = adultTrainImportance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "### Tune and compare 4 different classifiers. These are: Decision Trees (RF and XGBoost), SVM and NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoder\n",
    "le = preprocessing.LabelEncoder()\n",
    "# Test data\n",
    "Xtest = adultTest.values\n",
    "\n",
    "SEED = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Baseline dataset\n",
    "Xtrain = adultTrain.values\n",
    "Ytrain = le.fit_transform(targetTrain)\n",
    "\n",
    "# shape check\n",
    "print(Xtrain.shape)\n",
    "print(Xtest.shape)\n",
    "print(Ytrain.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baselineKnn = KNeighborsClassifier()\n",
    "baselineKnnAcc = cross_val_score(baselineKnn, Xtrain, Ytrain, cv=10, scoring='accuracy')\n",
    "baselineKnnAccMean = baselineKnnAcc.mean()\n",
    "print('mean accuracy for baseline knn: ', baselineKnnAccMean)\n",
    "\n",
    "currentBestModel = {\n",
    "    'model': copy.deepcopy(baselineKnn), \n",
    "    'cv': baselineKnnAccMean, \n",
    "    'X': Xtrain,\n",
    "    'Y': Ytrain\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=SEED)\n",
    "\n",
    "rfConfig = {\n",
    "    'n_estimators': np.arange(10, 50),\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': np.arange(5, 50)\n",
    "}\n",
    "\n",
    "rfRandomSearch = (\n",
    "    RandomizedSearchCV(\n",
    "        rf, \n",
    "        rfConfig, \n",
    "        verbose=True, \n",
    "        scoring='accuracy', \n",
    "        cv=5, \n",
    "        n_iter=5, \n",
    "        n_jobs=-1, # all cores\n",
    "        random_state=SEED\n",
    "    )\n",
    ")\n",
    "\n",
    "rfRandomSearch.fit(Xtrain, Ytrain)\n",
    "\n",
    "rfMean = rfRandomSearch.best_score_\n",
    "print('mean accuracy for rf: ', rfMean)\n",
    "\n",
    "if rfMean > currentBestModel['cv']:\n",
    "    currentBestModel = {\n",
    "        'model': rfRandomSearch,\n",
    "        'cv': rfMean\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(random_state=SEED, use_label_encoder=False)\n",
    "\n",
    "xgbConfig = {\n",
    "    'n_estimators': np.arange(10, 50),\n",
    "    'learning_rate': np.arange(1e-3, 1),\n",
    "    'max_depth': np.arange(5, 50),\n",
    "    'reg_alpha': [1e-5, 1e-2, 0.1, 1, 100],\n",
    "    'reg_lambda': [1e-5, 1e-2, 0.1, 1, 100]\n",
    "}\n",
    "\n",
    "xgbRandomSearch = (\n",
    "    RandomizedSearchCV(\n",
    "        xgb, \n",
    "        xgbConfig, \n",
    "        verbose=True, \n",
    "        cv=5, \n",
    "        n_iter=5, \n",
    "        n_jobs=-1, # all cores\n",
    "        random_state=SEED\n",
    "    )\n",
    ")\n",
    "\n",
    "xgbRandomSearch.fit(Xtrain, Ytrain)\n",
    "\n",
    "xgbMean = xgbRandomSearch.best_score_\n",
    "print('mean accuracy for xgb: ', xgbMean)\n",
    "\n",
    "if xgbMean > currentBestModel['cv']:\n",
    "    currentBestModel = {\n",
    "        'model': xgbRandomSearch,\n",
    "        'cv': xgbMean\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(random_state=SEED, probability=True)\n",
    "\n",
    "svmConfig = {\n",
    "    'C': np.arange(1e-3, 10),\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "svmRandomSearch = (\n",
    "    RandomizedSearchCV(\n",
    "        svm, \n",
    "        svmConfig, \n",
    "        verbose=True, \n",
    "        scoring='accuracy', \n",
    "        cv=5, \n",
    "        n_iter=5, \n",
    "        n_jobs=-1, # all cores\n",
    "        random_state=SEED\n",
    "    )\n",
    ")\n",
    "\n",
    "svmRandomSearch.fit(Xtrain, Ytrain)\n",
    "\n",
    "svmMean = svmRandomSearch.best_score_\n",
    "print('mean accuracy for svm: ', svmMean)\n",
    "\n",
    "if svmMean > currentBestModel['cv']:\n",
    "    currentBestModel = {\n",
    "        'model': svmRandomSearch,\n",
    "        'cv': svmMean\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model definition\n",
    "nn = MLPClassifier(random_state=SEED, early_stopping=True)\n",
    "\n",
    "# RandomizedSearchCV\n",
    "nnConfig = {\n",
    "    'hidden_layer_sizes': [(2 ** i,) for i in np.arange(2, 7)], # just one hidden layer\n",
    "    'alpha': [1e-10, 1e-8, 1e-6 1e-4, 1e-2, 1e-0, 1e2]\n",
    "    'learning_rate': ['constant', 'adaptive']\n",
    "}\n",
    "\n",
    "nnRandomSearch = (\n",
    "    RandomizedSearchCV(\n",
    "        nn, \n",
    "        nnConfig, \n",
    "        verbose=True, \n",
    "        scoring='accuracy', \n",
    "        cv=5, \n",
    "        n_iter=30, \n",
    "        n_jobs=-1, \n",
    "        random_state=SEED\n",
    "    )\n",
    ")\n",
    "\n",
    "nnRandomSearch.fit(Xtrain, Ytrain)\n",
    "\n",
    "nnMean = nnRandomSearch.best_score_\n",
    "print('mean accuracy for svm: ', nnMean)\n",
    "\n",
    "if nnMean > currentBestModel['cv']:\n",
    "    currentBestModel = {\n",
    "        'model': nnRandomSearch,\n",
    "        'cv': nnMean\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engineered datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### NOTEBOOK WAS TO SLOW WITH THIS, DECIDED TO COMMENT OUT ###################\n",
    "\n",
    "# # 1. Importance Sampling dataset\n",
    "# Ytrain70ImportanceNotEncoded = adultTrain70Importance.pop(\"target\").values\n",
    "# print('Ytrain70ImportanceNotEncoded:', Ytrain70ImportanceNotEncoded) \n",
    "# Ytrain70Importance = le.fit_transform(Ytrain70ImportanceNotEncoded)\n",
    "# print('Ytrain70Importance:', Ytrain70Importance) ## remove afterwards\n",
    "# Xtrain70Importance = adultTrain70Importance.values\n",
    "\n",
    "# # for cross validation \n",
    "# # k-fold cross validation is not done here because, the duplicated rows would leak to the cv sets\n",
    "# #Ytrain30ImportanceNotEncoded = adultTrain30Importance.pop(\"target\")\n",
    "# Ytrain30Importance = le.fit_transform(Ytrain30ImportanceNotEncoded)\n",
    "# Xtrain30Importance = adultTrain30Importance.values\n",
    "\n",
    "# # shape check\n",
    "# print(Xtrain70Importance.shape)\n",
    "# print(Xtest.shape) \n",
    "# print(Ytrain70Importance.shape)\n",
    "\n",
    "# # 5 is the deafult n_neighbors\n",
    "# importanceSamplingKnn = KNeighborsClassifier(n_neighbors=int(5*knnNeighboursFactor))\n",
    "# importanceSamplingKnn.fit(Xtrain70Importance, Ytrain70Importance)\n",
    "# Ytrain30Prediction = importanceSamplingKnn.predict(Xtrain30Importance)\n",
    "# print('Ytrain30Prediction', Ytrain30Prediction) ### remove afterwards\n",
    "# importanceSamplingKnnAccMean = accuracy_score(Ytrain30Importance, Ytrain30Prediction)\n",
    "\n",
    "# print('mean accuracy for importanceSamplingKnnAccMean: ', importanceSamplingKnnAccMean)\n",
    "# if importanceSamplingKnnAccMean > currentBestModel['cv']:  \n",
    "#     # get whole dataset fro promisingDatasets\n",
    "#     adultTrainImportance = promisingDatasets[\"importanceSampling\"] \n",
    "    \n",
    "#     YtrainImportance = le.fit_transform(adultTrainImportance.pop(\"target\").values)\n",
    "#     XtrainImportance = adultTrainImportance.values\n",
    "        \n",
    "#     currentBestModel = {\n",
    "#         'model': importanceSamplingKnn,\n",
    "#         'cv': importanceSamplingKnnAccMean,\n",
    "#         'X': XtrainImportance,\n",
    "#         'Y': YtrainImportance\n",
    "#     }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final model\n",
    "### Trained on entire train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = currentBestModel['model']\n",
    "X = currentBestModel['X']\n",
    "Y = currentBestModel['Y']\n",
    "model.fit(X, Y)\n",
    "predictions = model.predict(Xtest) # numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission\n",
    "### Save to csv in the required format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# going back to array of strings <=50 K and >50K\n",
    "predictions = le.inverse_transform(predictions)\n",
    "submissionDf = pd.DataFrame({'Id': idTest.values, 'income': predictions})\n",
    "\n",
    "submissionDf.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "66d7c0815ee3ec39f8a25542b36e8def4dce34ad6588e623c666e441b334d01c"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
